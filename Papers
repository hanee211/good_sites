> 목표 : IoT 환경의 센서 데이터를 활용해서 문장(글)을 생성한다.

> 예제 : 
1) 공기 중에 해로운 성분이 감지되면, 해로운 성분에 대한 수치를 알려주는 것이 아니라 말로 설명해준다. 
"지금 무슨 성분이 공기중에서 검출되었고, 그것은 몸의 어느 부분에 나쁘며 어떤 증상 유발이 예상된다." 라는 식으로.
2) 습도 감지 센서가 습도가 높아지는 추세를 감지하고 외출을 한면 비가 올 것 같다고 알려줌. 
3) 먼지가 많이 발생되면 청소를 해야 할 것 같다고 이야기함. 
4) 집에서 아이가 울고 있으면 엄마에게 아기가 울고 있다고 문자가 감.(아이를 재우고 외출 한다던지, 아니면 아이들끼리 노는 상황에서)

> 정리 : 
이 상황은 문장을 생성하는 기능에 특정한 context를 추가한 뒤 문장을 생성하는 것이다. 
즉 RNN 을 통해 학습한 machine(만들 SW 혹은 기능을  machine 이라 지칭) 에 어떤 단어를 넣어야 할지 골라야 함. 

하지만 일반 RNN 을 사용하는데 문제가 있음. 
그냥 shakespear 쓰기 처럼 사용되는 것이 아니라 

> 문제:
현재는 문장을 학습하여, 단어 다음의 단어 혹은 글자 다음의 글자를 예측해서 generate 하는 구조이다. 
그런데 만약 현재 있는 데이터를 이용하여 문장을 생성하고 싶으면?
이건 정말 어려운 일이 될 것이다. 
왜냐하면 일단 현재 문장을 deep learning 으로 학습한다는 것은 문장의 구조나 형태소 등의 분석이 전혀 없고
(있다한들 그것은 신경망 각 노드의 값일 뿐이지 의미가 있는 값은 아니다.)
그렇다면 현재 갖고 있는 데이터 온도는 25도 이고, 습도는 몇 %이니 우산을 준비하라고 하던지, 등의 데이터를 deep learning 된 
신경망에 전혀 사용할 수 가 없는 것이다. 

재미로(for FUN!!) 글을 쓰는 것은 가능하나, 원하는 데이터를 활용하여 글을 쓰게 할 수 있을까?

context 를 활용해서 글을 쓰는 논문은 나와있다 얼마나 효과가 있을지는 모르겠지만.
(CONTEXT DEPENDENT RECURRENT NEURAL NETWORK LANGUAGE MODEL )
https://www.semanticscholar.org/paper/Context-dependent-recurrent-neural-network-Mikolov-Zweig/04e0fefb859f4b02b017818915a2645427bfbdb2/pdf


그렇다면 현재 context 와 deep learning , RNN , NLP 등과 관련된 논문을 더 찾는다.
keyword : context, deep learning, RNN, NLP, readily scale, trainable generator 
oppsited keywordl : rule-based(templated-based) approache(Cheyer and Guzzoni, 2007), not easily scale to large open domain
  corpus-based methods

1. CONTEXT DEPENDENT RECURRENT NEURAL NETWORK LANGUAGE MODEL 
PAPERS : https://www.semanticscholar.org/paper/Context-dependent-recurrent-neural-network-Mikolov-Zweig/04e0fefb859f4b02b017818915a2645427bfbdb2/pdf

- to condition the hidden and output vectors on a continuous space representation of the previous words and sentecnes.
- Latent Dirichlet Allocation, we are able to avoid the data fragmentation associated with the traditional processing
building multiple topic-specific language models. 

- context vectors 
- contextual real-valued input vector in associated with each word. 
- Thie vector is used to convey contextual information about the sentence being modeled. 
- IMPORTANT PAPER


2. Recurrent neural network based language model(20, 7, 2010)
(http://www.fit.vutbr.cz/research/groups/speech/servite/2010/rnnlm_mikolov.pdf)
Tomas Mikolov

- General explanation of RNN and compare result between feedforward neural network and RNN
- denote many result about comparing between models and data


3. Multi-Context Recurrent Neural Network for Time Series Applications
B. Q. Huang, Tarik Rashid and M-T. Kechadi
http://waset.org/publications/3524/multi-context-recurrent-neural-network-for-time-series-applications

a multi-context recurrent network for time series analysis.
SRN(simply recurrent  network)  - shortcoming in terms of learning speed and accuracy 
=> to solve this problems, MCRN(multi-context recurrent network) 


4. Incorporating Side Information into Recurrent Neural Network Language Models
https://www.researchgate.net/profile/Vu_Hoang6/publication/299410055_Incorporating_Side_Information_into_Recurrent_Neural_Network_Language_Models/links/56f4a85008ae7c1fda2d78bf.pdf

여기까지 보면 대략 contextural information 을 가지고 RNN 을 하려는 시도가 있음.
다만 그 contextual information 의 detail level이 어느정도인지는 좀 더 조사가 필요함. 

5. Show and Tell: A Neural Image Caption Generator
https://arxiv.org/pdf/1411.4555.pdf

6. Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems
http://arxiv.org/pdf/1508.01745v2.pdf
-----------------------------------------------------------------------------------------------
NLG(natural language generation) 
 <Introduction>
- a good generator usually depends on several factors: adequacy, fluency, readability and variation.
- previous approaches
 1) ruled-based(or template-based) => robustness and adequacy but repetition, not easily scale to large open domain
 2) trainable generator => allow the model to adapt to different domains. but require a handcrafted generator to define
    the decision space within which statistics can be used for optimisation
 3) corpus-based method -> by defining a flexible learning structure, aim to learn generation directly from data by adopting
    over-generation and reranking paradigm
    in which final responses are obtained by reranking a set of candidates generated from a stochastic generator.
    
    => But!!, weaknesses in the areas of traning data efficiency, accuracy and naturalness
 
  ** this paper
  - present a statistical NLG based on a semantically controlled Long Short-term memory(LSTM) recurrent network.
  - it can learn from unaligned data by jointly optimising its sentence planning and surface realisation components using
    a simple cross entropy training criterion without any heuristics, and good quality language variation is obtained simply
    by randomly sampling the network outputs


 <Related work>
 - Conventional approaches to NLG typically divide the task into sentence planning and surface realisation
  1) Sentence planning
   Maps input semantic symbols into an intermediary form representing the utterance, e.g. a tree-like or template structure, 
  
  2) surface realisation
   convert the intermediate structure into the final text
   
  => although statistical sentence planning has been explored previously, generating the most likely context-free derivations
  given a corpus or maximising the expected reward using reinforcement learning, these methods still rely on a pre-existing, 
  handcrafted generator.
 
 - another approach #1
 : to minimisze handcrafting, 
 : Proposed Learning sentence planning rules directly from a corpus of uteerances lablled
 => expensive 
 
 - another approach #2
 : phrase-based NLG system based on factored LMs that can learn from a semantically aligned corpus.
 
 - another approache #3
 : neural network-based approaches(1982), RNNLM(recurrent neural network-based language model, 2010)
 => show the value of distributed representations and the ability to model arbitrarily long dependencies. 
 
 - another approahces #4
 - Sutskever et al.(2011) describes a simple variant of the RNN that can generate meaningful sentences by learning from a character-level corpus
 
 - another approaches #5
 Karpathy and Fei-Fei(2014) have demonstrated that an RNNNLM is capable of generating image descriptions by conditioning the network 
 model on a pre-trained convolutional image feature representation
 
 - another approahces #6
 Zhang and Lapata(2014) describes interesting work using RNNs to generate Chinese poetry.
 
-----------------------------------------------------------------------------------------------

7. LSTM based Conversation Models
http://arxiv.org/pdf/1603.09457v1.pdf


8. Reducing the Dimensionality of Data with Neural Networks
http://science.sciencemag.org/content/313/5786/504.full.pdf+html

9. Show, Attend and Tell: Neural Image Caption Generation with Visual Attention
http://www.jmlr.org/proceedings/papers/v37/xuc15.pdf
