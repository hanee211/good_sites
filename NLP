Add 10.28

CS 388: 
Natural Language Processing: 
Statistical Parsing

Raymond J. Mooney
University of Texas at Austin
---------------------------------------------


Probabilistic Context-Free Grammars (PCFGs)Michael Collins1
> http://www.cs.columbia.edu/~mcollins/courses/nlp2011/notes/pcfgs.pdf

What does PCFG mean or stand for ?
> http://meaningsfor.com/meaning-of/pcfg/pcfg-stands-for-pcfg-means

Probabilistic Context Grammars
> http://gawron.sdsu.edu/compling/course_core/lectures/pcfg/prob_parse.htm

{ProbabilisticjStochastic}Context-Free Grammars (PCFGs)
> http://nlp.stanford.edu/fsnlp/pcfg/fsnlp-pcfg-slides.pdf

CFG: Parsing
> http://web-ext.u-aizu.ac.jp/~hamada/AF/Compact-L5-Final-2.pdf

The Stanford Natural Language Processing GroupThe Stanford NLP Group	 
> http://nlp.stanford.edu/software/parser-faq.shtml

1. Explain about the constituency and dependency tree

  http://nlp.stanford.edu/software/stanford-dependencies.shtml
  
  
  1) Why do choose dependency rather than constituency for syntax : A Formal Point of View
    http://www.ruslang.ru/doc/melchuk_festschrift2012/Kahane.pdf
  
  2) Dependencies vs. Constituents for Tree-Based Alignment
    https://www.cs.rochester.edu/~gildea/gildea-emnlp04.pdf
  
  3) A Fundamental Algorithm for Dependency Parsing
    http://web.stanford.edu/~mjkay/covington.pdf
    
  4) Why is constituency needed, since dependency gets the job done more easily and economically?
    http://linguistics.stackexchange.com/questions/7280/why-is-constituency-needed-since-dependency-gets-the-job-done-more-easily-and-e
    
  5) description parsing in Korean.
  http://www.aistudy.co.kr/linguistics/natural/parsing.htm
  http://www.aistudy.co.kr/linguistics/natural/nlp_kim.htm

  6) Dependency tree
  http://www.ilc.cnr.it/EAGLES96/segsasg1/node44.html


  7) About DT, and general facts
  http://nlp.stanford.edu/~socherr/thesis.pdf
  
  8) RNN Tensorflow Code
  https://gist.github.com/j-min/481749dcb853b4477c4f441bf7452195
  
  9) Socher 
  http://nlp.stanford.edu/~socherr/thesis.pdf
   - Semantic Compositionality through Recursive Matrix-Vector Spaces
    http://www.aclweb.org/anthology/D12-1110
    
   - Parsing with Compositional Vector Grammars
    http://nlp.stanford.edu/pubs/SocherBauerManningNg_ACL2013.pdf
   
   - A Neural Network for Factoid Question Answering over Paragraphs
    https://cs.umd.edu/~miyyer/pubs/2014_qb_rnn.pdf
    
  
   
  10) ETC - Hopfield Network
  http://twinw.tistory.com/26
  



2. Natual Language Toolkit
http://www.nltk.org/
http://www.nltk.org/book/



3. Skip-gram
http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/
https://www.tensorflow.org/versions/r0.11/tutorials/word2vec/index.html

https://shuuki4.wordpress.com/2015/12/22/kaggle-whats-cooking/
https://shuuki4.wordpress.com/2016/01/27/word2vec-%EA%B4%80%EB%A0%A8-%EC%9D%B4%EB%A1%A0-%EC%A0%95%EB%A6%AC/
https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf

