* GAN 한글설명http://www.khshim.com/archives/218http://www.khshim.com/archives/20* Pixel RNN GAN 코드https://github.com/carpedm20/pixel-rnn-tensorflow* 관련 논문https://arxiv.org/pdf/1601.06759.pdfhttps://tensorflow.blog/tag/generative-adversarial-networks/https://www.quora.com/How-is-word2vec-different-from-the-RNN-encoder-decoderhttp://stackoverflow.com/questions/40166604/how-to-add-layers-to-rnn-encoder-decoder-network-in-seqtoseq-demo-for-paddle* Sequence to Sequence Model in Tensorflowhttps://www.tensorflow.org/tutorials/seq2seqRNN python 으로 구현하기 http://aikorea.org/blog/rnn-tutorial-2/GAN 설명 - 번역http://keunwoochoi.blogspot.kr/2016/12/generative-adversarial-network-gan.htmlhttp://jaejunyoo.blogspot.com/2017/01/generative-adversarial-nets-1.html* A Neural Attention Model for Sentence Summarizationhttps://www.aclweb.org/anthology/D/D15/D15-1044.pdfhttps://www.quora.com/What-is-exactly-the-attention-mechanism-introduced-to-RNN-recurrent-neural-network-It-would-be-nice-if-you-could-make-it-easy-to-understandhttps://indico.io/blog/sequence-modeling-neural-networks-part2-attention-models/https://www.quora.com/In-what-way-are-Adversarial-Networks-related-or-different-to-Adversarial-Training/answer/Ian-Goodfellow?__hstc=36392319.566e413a48a3eb7d0a2fee1f8154a4d7.1483080066931.1483080066931.1483082945743.2&__hssc=36392319.1.1483082945743&__hsfp=4265289821* Attention and Memory in Deep Learning and NLPhttp://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/* https://arxiv.org/pdf/1406.2661.pdfhttps://arxiv.org/pdf/1406.2661.pdf
